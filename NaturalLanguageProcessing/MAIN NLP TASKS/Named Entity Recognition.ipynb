{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "2yA34UawFJqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bxPIYrrwFAC4"
      },
      "outputs": [],
      "source": [
        "#@ DOWNLOAD REQUIRED LIBRARIES AND DEPENDENCIES\n",
        "%%bash\n",
        "pip install transformers[torch] -q\n",
        "pip install datasets tokenizers -q\n",
        "pip install seqeval -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INSTALLING THE REQUIRED LIBRARIES AND DEPENDENCIES\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "import numpy as np\n",
        "import datasets"
      ],
      "metadata": {
        "id": "UzgAlLk4FWQ4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ LOADING THE HUGGING FACE DATASETS\n",
        "conll_data = datasets.load_dataset(\"conll2003\")\n",
        "conll_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd4Z5FGQIzlJ",
        "outputId": "23f3db9b-e8d0-48d2-d038-d2061af6b592"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conll_data.shape                 # checking shape of the data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxZwTCj-JGrN",
        "outputId": "75432617-fa0b-4c61-9906-e079da03a42e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': (14041, 5), 'validation': (3250, 5), 'test': (3453, 5)}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's explore the train dataset\n",
        "print(conll_data[\"train\"][0])\n",
        "print(\"\\n The NER tags used in this datasets are: \\n\", conll_data[\"train\"].features[\"ner_tags\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk8Cb6WeJPWx",
        "outputId": "10954e59-bf3c-4b13-932e-b575062d7f53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n",
            "\n",
            " The NER tags used in this datasets are: \n",
            " Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conll_data[\"train\"].description)                 # description of the dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-olc3oQJfMq",
        "outputId": "73a494cd-9ecf-4e4b-d495-aa6c4c3eec08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\n",
            "four types of named entities: persons, locations, organizations and names of miscellaneous entities that do\n",
            "not belong to the previous three groups.\n",
            "\n",
            "The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\n",
            "a separate line and there is an empty line after each sentence. The first item on each line is a word, the second\n",
            "a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\n",
            "and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\n",
            "if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\n",
            "B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\n",
            "tagging scheme, whereas the original dataset uses IOB1.\n",
            "\n",
            "For more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Loading the BERT Tokenizer for tokenization\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "YtF8o84pKKX9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "\n",
        "- Transformers are often pretrained with subword tokenizers, meaning that even if your inputs have been split into words already, each of those words could be split again by the tokenizer.\n",
        "\n",
        "- This means that we need to do some processing on our labels as the input ids returned by the tokenizer are longer than the lists of labels our dataset contain.\n",
        "\n",
        "- This is happening, first because some special tokens might be added (we can a `[CLS]` and a `[SEP]` above) and then because of those possible splits of words in multiple tokens:"
      ],
      "metadata": {
        "id": "_CQ02qg3KfYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Inspecting output of variables\n",
        "conll_data[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri0-ACIEKe7a",
        "outputId": "9c7dd002-a6bf-401c-bdcc-b98cdbe83d56"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = conll_data[\"train\"][0]\n",
        "tokenized_input = tokenizer(example_text[\"tokens\"], is_split_into_words = True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "word_ids = tokenized_input.word_ids()\n",
        "\n",
        "print(word_ids)\n",
        "print(tokenized_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvEoyNoyKZSb",
        "outputId": "8bbdb195-539b-4d18-cb90-d7b5bd1a0e1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]\n",
            "{'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN-pccSNLUNO",
        "outputId": "aee3dfae-5532-450b-c3d0-726cf8b507b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'eu',\n",
              " 'rejects',\n",
              " 'german',\n",
              " 'call',\n",
              " 'to',\n",
              " 'boycott',\n",
              " 'british',\n",
              " 'lamb',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The input ids returned by the tokenizer are longer than the lists of labels our dataset contains"
      ],
      "metadata": {
        "id": "iw5VKDs724QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples, label_all_tokens = True):\n",
        "    \"\"\"\n",
        "    Tokenizes a list of examples and aligns corresponding labels to tokenized inputs.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): A dictionary containing the input tokens and corresponding labels.\n",
        "\n",
        "        label_all_tokens (bool): If True, assigns labels to all tokens in the input.\n",
        "        If False, assigns labels only to the first token of each word.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the tokenized inputs with aligned labels.\n",
        "        \"\"\"\n",
        "\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation = True, is_split_into_words = True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['ner_tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index = i)\n",
        "        previous_word_idx = None\n",
        "\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs['labels'] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "nuzhNfkQ2stg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll_data['train'][4:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32V5q9iG4mhF",
        "outputId": "f7e24389-87aa-4f41-a91f-d75aca18dcce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': ['4'],\n",
              " 'tokens': [['Germany',\n",
              "   \"'s\",\n",
              "   'representative',\n",
              "   'to',\n",
              "   'the',\n",
              "   'European',\n",
              "   'Union',\n",
              "   \"'s\",\n",
              "   'veterinary',\n",
              "   'committee',\n",
              "   'Werner',\n",
              "   'Zwingmann',\n",
              "   'said',\n",
              "   'on',\n",
              "   'Wednesday',\n",
              "   'consumers',\n",
              "   'should',\n",
              "   'buy',\n",
              "   'sheepmeat',\n",
              "   'from',\n",
              "   'countries',\n",
              "   'other',\n",
              "   'than',\n",
              "   'Britain',\n",
              "   'until',\n",
              "   'the',\n",
              "   'scientific',\n",
              "   'advice',\n",
              "   'was',\n",
              "   'clearer',\n",
              "   '.']],\n",
              " 'pos_tags': [[22,\n",
              "   27,\n",
              "   21,\n",
              "   35,\n",
              "   12,\n",
              "   22,\n",
              "   22,\n",
              "   27,\n",
              "   16,\n",
              "   21,\n",
              "   22,\n",
              "   22,\n",
              "   38,\n",
              "   15,\n",
              "   22,\n",
              "   24,\n",
              "   20,\n",
              "   37,\n",
              "   21,\n",
              "   15,\n",
              "   24,\n",
              "   16,\n",
              "   15,\n",
              "   22,\n",
              "   15,\n",
              "   12,\n",
              "   16,\n",
              "   21,\n",
              "   38,\n",
              "   17,\n",
              "   7]],\n",
              " 'chunk_tags': [[11,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   11,\n",
              "   12,\n",
              "   12,\n",
              "   11,\n",
              "   12,\n",
              "   12,\n",
              "   12,\n",
              "   12,\n",
              "   21,\n",
              "   13,\n",
              "   11,\n",
              "   12,\n",
              "   21,\n",
              "   22,\n",
              "   11,\n",
              "   13,\n",
              "   11,\n",
              "   1,\n",
              "   13,\n",
              "   11,\n",
              "   17,\n",
              "   11,\n",
              "   12,\n",
              "   12,\n",
              "   21,\n",
              "   1,\n",
              "   0]],\n",
              " 'ner_tags': [[5,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   3,\n",
              "   4,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   1,\n",
              "   2,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   5,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Let's apply the function to the whole dataset\n",
        "tokenized_datasets = conll_data.map(tokenize_and_align_labels, batched = True)\n",
        "tokenized_datasets['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzVNycN_5c4h",
        "outputId": "498d6d9e-9d13-4ab9-902c-2d1db4d5037e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
              " 'input_ids': [101,\n",
              "  7327,\n",
              "  19164,\n",
              "  2446,\n",
              "  2655,\n",
              "  2000,\n",
              "  17757,\n",
              "  2329,\n",
              "  12559,\n",
              "  1012,\n",
              "  102],\n",
              " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ DEFINING THE MODEL\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels = 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kew8hcIA621-",
        "outputId": "f7359f2a-639c-4e57-edae-19a4ee631d5f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ DEFINING THE TRAINING ARGUMENTS\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"test-ner\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate = 2e-5,\n",
        "    per_device_train_batch_size = 16,\n",
        "    per_device_eval_batch_size = 16,\n",
        "    num_train_epochs = 3,\n",
        "    weight_decay = 0.01,\n",
        ")"
      ],
      "metadata": {
        "id": "OnfWoMk77TzS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INSTANTIATING THE DATA COLLATOR AND EVALUATION METRICS\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = datasets.load_metric(\"seqeval\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyx5p6Eq7vBl",
        "outputId": "888ef487-4745-47f8-f7ac-7b79afa5409f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-b75f234291f9>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = datasets.load_metric(\"seqeval\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Let's test the metric on example\n",
        "example = conll_data['train'][0]\n",
        "label_list = conll_data[\"train\"].features[\"ner_tags\"].feature.names"
      ],
      "metadata": {
        "id": "C8CkycQZ8hvc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in example[\"ner_tags\"]:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH78TkSr8tjX",
        "outputId": "5a35b346-94ce-4d36-c348-c22d8acd0992"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "0\n",
            "7\n",
            "0\n",
            "0\n",
            "0\n",
            "7\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [label_list[i] for i in example[\"ner_tags\"]]"
      ],
      "metadata": {
        "id": "qNwyeyrb829m"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the metrics\n",
        "metric.compute(predictions = [labels], references = [labels])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EOYRatq9dVQ",
        "outputId": "657ea9b3-5ecb-42dd-d322-b063e2bc0284"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
              " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
              " 'overall_precision': 1.0,\n",
              " 'overall_recall': 1.0,\n",
              " 'overall_f1': 1.0,\n",
              " 'overall_accuracy': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"\n",
        "    Computes evaluation metrics for named entity recognition (NER) tasks based on predicted logits and true labels.\n",
        "\n",
        "    Args:\n",
        "        eval_preds (tuple): A tuple containing two lists: predicted logits and true labels.\n",
        "            - Predicted logits: A list of lists of float values representing the model's predictions.\n",
        "            - True labels: A list of lists of integers representing the ground truth labels.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing computed metrics, including precision, recall, F1 score, and accuracy.\n",
        "    \"\"\"\n",
        "    pred_logits, labels = eval_preds\n",
        "\n",
        "    pred_logits = np.argmax(pred_logits, axis=2)\n",
        "\n",
        "    # Filter out labels with the special value -100\n",
        "    predictions = [\n",
        "        [label_list[eval_pred] for eval_pred, l in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(pred_logits, labels)\n",
        "    ]\n",
        "\n",
        "    true_labels = [\n",
        "        [label_list[l] for eval_pred, l in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(pred_logits, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=predictions, references=true_labels)\n",
        "\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n"
      ],
      "metadata": {
        "id": "4ylVfp-H9d13"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INSTANTIATING THE TRAINER\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset = tokenized_datasets['train'],\n",
        "    eval_dataset = tokenized_datasets['validation'],\n",
        "    data_collator = data_collator,\n",
        "    tokenizer = tokenizer,\n",
        "    compute_metrics = compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "AlpggzUF-BUw",
        "outputId": "22ee56e2-5c55-4425-bd94-b7dd69648f87"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2634/2634 08:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.221400</td>\n",
              "      <td>0.065681</td>\n",
              "      <td>0.916310</td>\n",
              "      <td>0.933326</td>\n",
              "      <td>0.924740</td>\n",
              "      <td>0.982160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.044900</td>\n",
              "      <td>0.056375</td>\n",
              "      <td>0.931271</td>\n",
              "      <td>0.939814</td>\n",
              "      <td>0.935523</td>\n",
              "      <td>0.985162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.026600</td>\n",
              "      <td>0.058532</td>\n",
              "      <td>0.934470</td>\n",
              "      <td>0.944401</td>\n",
              "      <td>0.939409</td>\n",
              "      <td>0.985702</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ SAVING THE MODEL AND TOKENIZER\n",
        "model.save_pretrained(\"/content/drive/MyDrive/Projects/Fine Tuning BERT for NER/ner_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Projects/Fine Tuning BERT for NER/tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4pXnlBa-VoA",
        "outputId": "abc94ed7-c57a-49ab-eeeb-0f85bf4db1c6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Projects/Fine Tuning BERT for NER/tokenizer/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Projects/Fine Tuning BERT for NER/tokenizer/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Projects/Fine Tuning BERT for NER/tokenizer/vocab.txt',\n",
              " '/content/drive/MyDrive/Projects/Fine Tuning BERT for NER/tokenizer/added_tokens.json',\n",
              " '/content/drive/MyDrive/Projects/Fine Tuning BERT for NER/tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    str(i): label for i, label in enumerate(label_list)\n",
        "}\n",
        "\n",
        "print(id2label)\n",
        "\n",
        "label2id = {\n",
        "    label: str(i) for i, label in enumerate(label_list)\n",
        "}\n",
        "print(label2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyCWqW74CaYH",
        "outputId": "0ea2acf8-da03-4d24-e585-d3af6179a1b2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'0': 'O', '1': 'B-PER', '2': 'I-PER', '3': 'B-ORG', '4': 'I-ORG', '5': 'B-LOC', '6': 'I-LOC', '7': 'B-MISC', '8': 'I-MISC'}\n",
            "{'O': '0', 'B-PER': '1', 'I-PER': '2', 'B-ORG': '3', 'I-ORG': '4', 'B-LOC': '5', 'I-LOC': '6', 'B-MISC': '7', 'I-MISC': '8'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ LOADING THE MODEL\n",
        "import json\n",
        "config = json.load(open(\"/content/drive/MyDrive/Projects/Fine Tuning BERT for NER/ner_model/config.json\"))\n",
        "config[\"id2label\"] = id2label\n",
        "config[\"label2id\"] = label2id\n",
        "json.dump(config, open(\"/content/drive/MyDrive/Projects/Fine Tuning BERT for NER/ner_model/config.json\", \"w\"))"
      ],
      "metadata": {
        "id": "kBihe18DDCvo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ LOADING THE FINETUNED CLASSIFICATION MODEL\n",
        "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/Projects/Fine Tuning BERT for NER/ner_model\")"
      ],
      "metadata": {
        "id": "H9RbdlJJDGKT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Let's do some prediction\n",
        "from transformers import pipeline\n",
        "nlp = pipeline(\"ner\",\n",
        "               model = model_fine_tuned,\n",
        "               tokenizer = tokenizer,\n",
        "               )\n",
        "\n",
        "example = \"Sundar Pichai is CEO of Google\"\n",
        "ner_results = nlp(example)\n",
        "print(ner_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thvjkjkRDwDb",
        "outputId": "1c028c17-6240-455d-8d43-fe54c6bf69a8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity': 'B-PER', 'score': 0.99862564, 'index': 1, 'word': 'sun', 'start': 0, 'end': 3}, {'entity': 'B-PER', 'score': 0.99862754, 'index': 2, 'word': '##dar', 'start': 3, 'end': 6}, {'entity': 'I-PER', 'score': 0.99837035, 'index': 3, 'word': 'pic', 'start': 7, 'end': 10}, {'entity': 'I-PER', 'score': 0.99800485, 'index': 4, 'word': '##hai', 'start': 10, 'end': 13}, {'entity': 'B-ORG', 'score': 0.98913145, 'index': 8, 'word': 'google', 'start': 24, 'end': 30}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The END**"
      ],
      "metadata": {
        "id": "VXyZg6YiE26R"
      }
    }
  ]
}