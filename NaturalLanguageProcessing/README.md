This section contains learning of Natural Language Processing (NLP) through the courses provided by [Hugging Face](https://huggingface.co/learn/nlp-course/chapter0/1) ðŸ¤—.

1. **[Transformer Models](https://github.com/regmi-saugat/HuggingFace/tree/main/01.%20Transformer%20Models):**
- Here, I have performed the tasks like *Sentiment Analysis, Zero-Shot Classification, Text Generation, Mask Filling, Named Entity Recognition, Question Answering, Summarization and Language Translation.*


2. **[Hugging Face Transformers](https://github.com/regmi-saugat/HuggingFace/tree/main/02.%20Hugging%20Face%20Transformers)**
- This contains the major steps required in the pipelining process which is preprocessing the tokenizer, passing inputs through the model, and postprocessing the output. The whole pipeline serves as a well structured path which transforms raw text into actionable insights.


3. **[Fine Tuning Pretrained Model](https://github.com/regmi-saugat/HuggingFace/tree/main/03.%20Fine%20Tuninig%20Pretrained%20Model)**
- This notebook presents about fine tuning the pre-trained model. Here, I have performed the fine tuning using using MRPC `GLUE` dataset. It contains information about Preprocessing the Dataset, Dynamic Padding, Training & Evaluation of ðŸ¤— Transformer Models.

